{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Predict with pre-trained AlphaPose Estimation models\n",
        "\n",
        "This article shows how to play with pre-trained Alpha Pose models with only a few\n",
        "lines of code.\n",
        "\n",
        "First let's import some necessary libraries:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Unable to import modules due to missing `mxnet` & `torch`. You should install at least one deep learning framework.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgluoncv\u001b[39;00m \u001b[39mimport\u001b[39;00m model_zoo, data, utils\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgluoncv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpose\u001b[39;00m \u001b[39mimport\u001b[39;00m detector_to_alpha_pose, heatmap_to_coord_alpha_pose\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\gluoncv\\__init__.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39many\u001b[39m((_found_mxnet, _found_pytorch)):\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnable to import modules due to missing `mxnet` & `torch`. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     34\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mYou should install at least one deep learning framework.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m((_found_mxnet, _found_pytorch)):\n\u001b[0;32m     37\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n",
            "\u001b[1;31mImportError\u001b[0m: Unable to import modules due to missing `mxnet` & `torch`. You should install at least one deep learning framework."
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from gluoncv import model_zoo, data, utils\n",
        "from gluoncv.data.transforms.pose import detector_to_alpha_pose, heatmap_to_coord_alpha_pose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a pretrained model\n",
        "\n",
        "Let's get a Alpha Pose model trained with input images of size 256x192 on MS COCO\n",
        "dataset. We pick the one using ResNet-101 V1b as the base model. By specifying\n",
        "``pretrained=True``, it will automatically download the model from the model\n",
        "zoo if necessary. For more pretrained models, please refer to\n",
        ":doc:`../../model_zoo/index`.\n",
        "\n",
        "Note that a Alpha Pose model takes a top-down strategy to estimate\n",
        "human pose in detected bounding boxes from an object detection model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "detector = model_zoo.get_model('yolo3_mobilenet1.0_coco', pretrained=True)\n",
        "pose_net = model_zoo.get_model('alpha_pose_resnet101_v1b_coco', pretrained=True)\n",
        "\n",
        "# Note that we can reset the classes of the detector to only include\n",
        "# human, so that the NMS process is faster.\n",
        "\n",
        "detector.reset_class([\"person\"], reuse_weights=['person'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-process an image for detector, and make inference\n",
        "\n",
        "Next we download an image, and pre-process with preset data transforms. Here we\n",
        "specify that we resize the short edge of the image to 512 px. But you can\n",
        "feed an arbitrarily sized image.\n",
        "\n",
        "This function returns two results. The first is a NDArray with shape\n",
        "``(batch_size, RGB_channels, height, width)``. It can be fed into the\n",
        "model directly. The second one contains the images in numpy format to\n",
        "easy to be plotted. Since we only loaded a single image, the first dimension\n",
        "of `x` is 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "im_fname = utils.download('https://github.com/dmlc/web-data/blob/master/' +\n",
        "                          'gluoncv/pose/soccer.png?raw=true',\n",
        "                          path='soccer.png')\n",
        "x, img = data.transforms.presets.yolo.load_test(im_fname, short=512)\n",
        "print('Shape of pre-processed image:', x.shape)\n",
        "\n",
        "class_IDs, scores, bounding_boxs = detector(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process tensor from detector to keypoint network\n",
        "\n",
        "Next we process the output from the detector.\n",
        "\n",
        "For a Alpha Pose network, it expects the input has the size 256x192,\n",
        "and the human is centered. We crop the bounding boxed area\n",
        "for each human, and resize it to 256x192, then finally normalize it.\n",
        "\n",
        "In order to make sure the bounding box has included the entire person,\n",
        "we usually slightly upscale the box size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pose_input, upscale_bbox = detector_to_alpha_pose(img, class_IDs, scores, bounding_boxs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predict with a Alpha Pose network\n",
        "\n",
        "Now we can make prediction.\n",
        "\n",
        "A Alpha Pose network predicts the heatmap for each joint (i.e. keypoint).\n",
        "After the inference we search for the highest value in the heatmap and map it to the\n",
        "coordinates on the original image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_heatmap = pose_net(pose_input)\n",
        "pred_coords, confidence = heatmap_to_coord_alpha_pose(predicted_heatmap, upscale_bbox)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display the pose estimation results\n",
        "\n",
        "We can use :py:func:`gluoncv.utils.viz.plot_keypoints` to visualize the\n",
        "results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = utils.viz.plot_keypoints(img, pred_coords, confidence,\n",
        "                              class_IDs, bounding_boxs, scores,\n",
        "                              box_thresh=0.5, keypoint_thresh=0.2)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
